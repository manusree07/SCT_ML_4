# Task 4: Hand Gesture Recognition using CNN

# 1. Import necessary libraries
import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import confusion_matrix
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import zipfile
import random
import pickle
from google.colab import files

# 2. Download and extract dataset
!wget -O asl_digits.zip "https://github.com/ardamavi/Sign-Language-Digits-Dataset/archive/refs/heads/master.zip"

with zipfile.ZipFile("asl_digits.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/")

dataset_path = "/content/Sign-Language-Digits-Dataset-master/Dataset"
gesture_classes = os.listdir(dataset_path)
print("Detected gesture classes:", gesture_classes)

# 3. Load and preprocess images
IMG_SIZE = 64
data = []
labels = []

print("ðŸ“¥ Loading and preprocessing images...")
for gesture in gesture_classes:
    class_path = os.path.join(dataset_path, gesture)
    for img_name in tqdm(os.listdir(class_path), desc=f"Loading {gesture}"):
        img_path = os.path.join(class_path, img_name)
        try:
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            data.append(img)
            labels.append(gesture)
        except:
            continue

data = np.array(data, dtype="float32") / 255.0
data = np.expand_dims(data, axis=-1)  # Add channel dimension
labels = np.array(labels)

# 4. Encode labels and split dataset
lb = LabelBinarizer()
labels_encoded = lb.fit_transform(labels)
print("Classes detected:", lb.classes_)

X_train, X_test, y_train, y_test = train_test_split(
    data, labels_encoded,
    test_size=0.2,
    random_state=42,
    stratify=labels_encoded
)

print("Train set shape:", X_train.shape, y_train.shape)
print("Test set shape:", X_test.shape, y_test.shape)

# 5. Define CNN architecture
model = Sequential()
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 1)))
model.add(MaxPooling2D((2,2)))
model.add(Conv2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))
model.add(Conv2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(gesture_classes), activation='softmax'))

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# 6. Train the CNN model
EPOCHS = 15
BATCH_SIZE = 32

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    verbose=1
)

# 7. Plot training & validation accuracy and loss
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='green')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss', color='green')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# 8. Evaluate model
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"ðŸŽ¯ Test Accuracy: {test_acc*100:.2f}%")

y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=gesture_classes, yticklabels=gesture_classes)
plt.title("Confusion Matrix - Hand Gesture Recognition")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# 9. Display 9 random sample predictions
plt.figure(figsize=(10,8))
for i in range(9):
    idx = np.random.randint(0, len(X_test))
    img = X_test[idx].reshape(64, 64)
    true_label = gesture_classes[y_true[idx]]
    pred_label = gesture_classes[y_pred[idx]]
    plt.subplot(3,3,i+1)
    plt.imshow(img, cmap='gray')
    color = 'green' if true_label == pred_label else 'red'
    plt.title(f"T:{true_label}\nP:{pred_label}", color=color)
    plt.axis('off')
plt.suptitle("Sample Predictions (Green=Correct, Red=Wrong)", fontsize=16)
plt.show()

# 10. Save model and label encoder
model.save("hand_gesture_cnn_model.h5")
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(lb, f)
print("âœ… Model and label encoder saved successfully")

# 11. Real-time prediction example (upload image in Colab)
uploaded = files.upload()
for file_name in uploaded.keys():
    img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)
    img_resized = cv2.resize(img, (64,64))
    img_norm = img_resized.astype('float32') / 255.0
    img_input = np.expand_dims(img_norm, axis=(0,-1))
    pred_prob = model.predict(img_input)
    pred_class = lb.classes_[np.argmax(pred_prob)]
    plt.imshow(img, cmap='gray')
    plt.title(f"Predicted Gesture: {pred_class}")
    plt.axis('off')
    plt.show()
